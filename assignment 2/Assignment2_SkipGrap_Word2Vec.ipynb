{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_SkipGrap_Word2Vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "es41Fx-j4Bbe"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BT3H7_X_ERQ",
        "outputId": "c33168fd-c533-4908-ea46-89a970adfc4d"
      },
      "source": [
        "#Necessary Library\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "#Import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(10)\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD17Hujsc2QJ"
      },
      "source": [
        "##Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnnoQkRL-KAG"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(corpus):\n",
        "    result = []\n",
        "    for i in corpus:\n",
        "        i = gensim.utils.simple_preprocess(i, deacc=True) #Remove Punctuation\n",
        "        i = \" \".join(i)\n",
        "        out = nltk.word_tokenize(i)\n",
        "        out = [x.lower() for x in out]\n",
        "        out = [x for x in out if x not in stop_words]\n",
        "        result.append(\" \". join(out))\n",
        "    return result\n",
        "\n",
        "def create_vocabulary(corpus):\n",
        "    '''Creates a dictionary with all unique words in corpus with id'''\n",
        "    id2word = {}\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    for d in corpus:\n",
        "        for w in d.split():\n",
        "            if w not in vocabulary:\n",
        "                vocabulary[w] = i\n",
        "                i+=1\n",
        "                id2word[i] = w\n",
        "    return vocabulary, id2word\n",
        "\n",
        "def prepare_dataset(corpus, window_size):\n",
        "  '''\n",
        "  neighbor to look at each direction\n",
        "  total neighbor is neighbor*2, window size is neighbor*2+1\n",
        "  '''\n",
        "  neighbor = window_size//2\n",
        "  total_neighbor = neighbor*2\n",
        "\n",
        "  columns = ['Input', 'Output']\n",
        "  \n",
        "\n",
        "  all_row = []\n",
        "\n",
        "  for doc in corpus:\n",
        "      for i, w in enumerate(doc.split()):\n",
        "          inp = w\n",
        "          for n in range(1, neighbor+1):\n",
        "\n",
        "              #look left\n",
        "              if (i-n) >= 0:\n",
        "                out = doc.split()[i-n]\n",
        "\n",
        "              #look right\n",
        "              if (i+n)<len(doc.split()):\n",
        "                out = doc.split()[i+n]\n",
        "\n",
        "              row = [inp,out]\n",
        "              all_row.append(row)\n",
        "  dataset = pd.DataFrame(all_row, columns=columns)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvzRaPVpqJGU"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9spkIO9e83aL"
      },
      "source": [
        "df = pd.read_csv('Review_word2vec_v1.csv')\n",
        "data = df[\"Text\"].values.tolist()\n",
        "\n",
        "#Set Corpus\n",
        "corpus=data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZSp3bPqOl5"
      },
      "source": [
        "##Start Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YYssKTQZAU7"
      },
      "source": [
        "#Hyper-parameter\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "window = 3\n",
        "embedding_dim = 50\n",
        "batch = 128\n",
        "num_epochs = 5\n",
        "load_model = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T0glYWjRW82"
      },
      "source": [
        "corpus = preprocess(corpus)\n",
        "vocabulary, id2word = create_vocabulary(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DZ9U5Z1_aFH"
      },
      "source": [
        "train_data = prepare_dataset(corpus, window_size=window)\n",
        "\n",
        "#Replace word with idx\n",
        "train_data.Input = train_data.Input.map(vocabulary)\n",
        "train_data.Output = train_data.Output.map(vocabulary)\n",
        "# print(train_data.head())\n",
        "\n",
        "\n",
        "train_data_input_loaded = DataLoader(train_data.Input.values, batch_size=batch) #.values Return a Numpy representation of the DataFrame.\n",
        "train_data_output_loaded = DataLoader(train_data.Output.values, batch_size=batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQqJAKZpZmGc"
      },
      "source": [
        "#More Helper\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "def convert_one_hot_tensor(tensor):\n",
        "    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n",
        "    size = [*tensor.shape][0]\n",
        "    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n",
        "    return inp.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqGlQGpWgKbW"
      },
      "source": [
        "class SG(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(SG, self).__init__()\n",
        "    self.FC1 = nn.Linear(vocab_size, embedding_dim)\n",
        "    self.FC2 = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.FC1(x))\n",
        "    x = self.FC2(x)\n",
        "    return x\n",
        "\n",
        "def save_checkpoint(checkpoint, filename=\"my_checkpoint.pth.tar\"):\n",
        "  # print(\"=> Saving Checkpoint\")\n",
        "  torch.save(checkpoint, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "  # print(\"=> Loading Checkpoint\")\n",
        "  model.load_state_dict(checkpoint[\"model\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEQZEt6ti8GF"
      },
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "model = SG(vocab_size, embedding_dim ).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9HVj5xBMFuc"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHx9_xxEjgov",
        "outputId": "e4deffeb-c70a-4daf-a377-9eab164b248a"
      },
      "source": [
        "if load_model:\n",
        "  load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  if epoch%2==0:\n",
        "    checkpoint ={'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "  for x, y in zip(train_data_input_loaded, train_data_output_loaded):\n",
        "    x = convert_one_hot_tensor(x)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(x)\n",
        "    loss = loss_function(scores, y)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    #weight update SGD\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  if epoch%2==0:\n",
        "    print(f'Epoch: {epoch} Loss: {total_loss}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 153287.1627960205\n",
            "Epoch: 2 Loss: 140333.50424957275\n",
            "Epoch: 4 Loss: 133264.14703845978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab9FJCJiB4D8"
      },
      "source": [
        "#Load Trained File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBDGm9BLrh8v",
        "outputId": "e3cd7529-9b3d-434b-8918-2ce642f7c97d"
      },
      "source": [
        "vocab_size = 39859\n",
        "embedding_dim = 50\n",
        "\n",
        "checkpoint = torch.load(\"my_checkpoint.pth.tar\")\n",
        "model = checkpoint[\"model\"]\n",
        "optimizer = checkpoint[\"optimizer\"]\n",
        "\n",
        "print(model['FC1.weight'].size())\n",
        "print(model['FC2.weight'].size())\n",
        "\n",
        "word_vectors = model['FC1.weight']\n",
        "\n",
        "word_vectors = np.transpose(word_vectors.cpu().numpy())\n",
        "\n",
        "print(f'Vacabulary Size: {vocab_size} Embedding Dimension: {embedding_dim}')\n",
        "print(word_vectors.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 39859])\n",
            "torch.Size([39859, 50])\n",
            "Vacabulary Size: 39859 Embedding Dimension: 50\n",
            "(39859, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACjYuRi0TvZJ"
      },
      "source": [
        "#Convert words to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GINWeIQQsbMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ba0d89-da55-4ab3-f4de-b3036e13351d"
      },
      "source": [
        "words = [\"Coffee\", \"Pasta\" ,\"Tuna\", \"Cookies\"]\n",
        "top = 10\n",
        "\n",
        "word_idx = [vocabulary[word.lower()] for word in words]\n",
        "\n",
        "words_hot = []\n",
        "for id in word_idx:\n",
        "  x = np.zeros(vocab_size)\n",
        "  x[id] = 1\n",
        "\n",
        "  words_hot.append(x)\n",
        "words_hot = torch.tensor(words_hot, device=device).float()\n",
        "\n",
        "W1 = torch.transpose(model['FC1.weight'], 0, 1).float()\n",
        "W2 = torch.transpose(model['FC2.weight'],0, 1).float()\n",
        "\n",
        "h = words_hot.mm(W1)\n",
        "y_pred = h.mm(W2)\n",
        "\n",
        "res_val, res_ind = y_pred.sort(descending=True, dim=1)\n",
        "res_ind = res_ind[:][:top]\n",
        "\n",
        "# res_arg = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "# res_idx = res_arg.cpu().numpy()\n",
        "\n",
        "res_idx = res_ind.cpu().numpy()\n",
        "for i in range(len(res_idx)):\n",
        "  print(f'Top 10 word for {words[i]}')\n",
        "  for j in range(top):\n",
        "    print(id2word[res_idx[i][j]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 word for Coffee\n",
            "craving\n",
            "items\n",
            "quality\n",
            "stores\n",
            "mints\n",
            "looks\n",
            "thanks\n",
            "trips\n",
            "clock\n",
            "great\n",
            "Top 10 word for Pasta\n",
            "feet\n",
            "althoug\n",
            "coconout\n",
            "cashier\n",
            "drown\n",
            "lime\n",
            "riddled\n",
            "considered\n",
            "unseasoned\n",
            "amsterdan\n",
            "Top 10 word for Tuna\n",
            "sudorific\n",
            "crisp\n",
            "numb\n",
            "packers\n",
            "undertones\n",
            "grinder\n",
            "sometimes\n",
            "incur\n",
            "languages\n",
            "determination\n",
            "Top 10 word for Cookies\n",
            "present\n",
            "craving\n",
            "trees\n",
            "cup\n",
            "clean\n",
            "easy\n",
            "straw\n",
            "quality\n",
            "items\n",
            "since\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B-JeJCb46PX"
      },
      "source": [
        "#Word Analogy with Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qwjbRmgcyuq",
        "outputId": "b836a4e5-8800-42dd-d239-e61b96110953"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-21 03:43:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-21 03:43:11--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 40s  \n",
            "\n",
            "2021-09-21 03:45:51 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e09Yqfwc4Vs",
        "outputId": "789163c9-35f8-4f46-b995-aac27b5e30c1"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJCYFmqkCNsA"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-HYc29MCRL6"
      },
      "source": [
        "def read_data(file_name):\n",
        "    with open(file_name,'r') as f:\n",
        "        vocabluary = set()\n",
        "        word2vec = {}\n",
        "\n",
        "        for line in f:\n",
        "          line = line.strip() #remove unecessary whitespce in the begining\n",
        "          row = line.split()\n",
        "          word = row[0] #first element is word\n",
        "          vocabluary.add(word)\n",
        "          word2vec[word] = np.array(row[1:], dtype=float)\n",
        "    return vocabluary, word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8lwzK-CCUKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03358af7-f539-4d56-f616-d155c23d5d85"
      },
      "source": [
        "vocab, w2v = read_data(\"./glove.6B.300d.txt\")\n",
        "print(f'Vocabulary Size: {len(vocab)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hy3RC6xCU7P"
      },
      "source": [
        "def cos_sim(u,v):\n",
        "    \"\"\"\n",
        "    u: vector of 1st word\n",
        "    v: vector of 2nd Word\n",
        "    \"\"\"\n",
        "    numerator_ = u.dot(v)\n",
        "    denominator_= np.sqrt(np.sum(np.square(u))) * np.sqrt(np.sum(np.square(v)))\n",
        "    return numerator_/denominator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBgC8WR3DRt5"
      },
      "source": [
        "def find_word4(word1, word2, word3):\n",
        "    word4 = \"\"\n",
        "\n",
        "    word1, word2, word3 = word1.lower(), word2.lower(), word3.lower()\n",
        "    diffVec = w2v[word3] - (w2v[word1] - w2v[word2])#word1 - word2 = word3 - word4\n",
        "\n",
        "    max_sim = -100000\n",
        "    for word in vocab:\n",
        "      vec = w2v[word]\n",
        "      sim_ = cos_sim(diffVec, vec)\n",
        "      if sim_> max_sim:\n",
        "        max_sim = sim_\n",
        "        word4 = word\n",
        "\n",
        "    return word4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8WFqI6eFoe7",
        "outputId": "0fc2110b-f8de-45e0-c60a-dfa8f7831f53"
      },
      "source": [
        "print(\"Spain is to Spanish as Germany is to\"+find_word4('Spain','Spanish','Germany'))\n",
        "print(\"Japan is to Tokyo as France is to \"+find_word4('Japan','Tokyo','France'))\n",
        "print(\"Woman is to Man as Queen is to \"+find_word4('Woman','Man','Queen'))\n",
        "print(\"Australia is to Hotdog as Italy is to \"+find_word4('Australia','Hotdog','Italy'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spain is to Spanish as Germany is togerman\n",
            "Japan is to Tokyo as France is to paris\n",
            "Woman is to Man as Queen is to queen\n",
            "Australia is to Hotdog as Italy is to hotdog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faLkXzLn4JAF"
      },
      "source": [
        "[1. SkipGram Intution Chris Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "[2. PyTorch Tutorial CBOW and N-gram](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
        "\n",
        "[3. Skipgram Explained with Code (***)](https://www.kaggle.com/karthur10/skip-gram-implementation-with-pytorch-step-by-step#Skip-Gram-example-with-PyTorch)\n",
        "\n",
        "[4. Skipgram and CBOW Pytorch clean code (***)](https://srijithr.gitlab.io/post/word2vec/)\n"
      ]
    }
  ]
}